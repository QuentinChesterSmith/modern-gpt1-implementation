model:
  num_layers: 8
  embed_dim: 192
  num_heads: 4
  vocab_size: 50257

optimization:
  optimizer: Adam
  lr: 0.0001

hyperparameters:
  epochs: 10
  seq_len: 512
  batch_size: 4

loss:
  loss_func: CrossEntropyLoss

logging_freq:
  train_loss: 500