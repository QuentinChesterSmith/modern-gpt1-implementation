model:
  num_layers: 8
  embed_dim: 192
  num_heads: 12
  vocab_size: 50257

optimization:
  optimizer: Adafactor
  lr: 0.0001

hyperparameters:
  epochs: 50
  seq_len: 512
  batch_size: 32

loss:
  loss_func: CrossEntropyLoss

logging_freq:
  train_loss: 50
  checkpoint: 2000