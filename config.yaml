model:
  num_layers: 8
  embed_dim: 192
  num_heads: 12
  vocab_size: 50257

optimization:
  optimizer: AdamW
  lr: 0.00003
  linear_scheduler_steps: 200 

hyperparameters:
  epochs: 50
  seq_len: 128
  batch_size: 4

loss:
  loss_func: CrossEntropyLoss

logging_freq:
  train_loss: 50
  checkpoint: 2000